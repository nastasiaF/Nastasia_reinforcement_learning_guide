{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training with greedy algorithm.\n"
   ],
   "metadata": {
    "id": "5GLPgVs-t_RD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vKOVSxN3sKvu",
    "ExecuteTime": {
     "end_time": "2023-10-15T16:28:39.964517500Z",
     "start_time": "2023-10-15T16:28:38.881763500Z"
    }
   },
   "outputs": [],
   "source": [
    "from Hangman_env import HangedManEnv\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's start an Environment"
   ],
   "metadata": {
    "id": "AJwo73Olyx5Q"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97ifKg2ssKvx",
    "outputId": "0b0d5a49-bb40-4d57-94f9-c57303e34f45",
    "ExecuteTime": {
     "end_time": "2023-10-15T16:30:13.351420500Z",
     "start_time": "2023-10-15T16:28:48.741569200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\nastasia.fouret\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty_like() missing 1 required positional argument: 'prototype'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m max_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m\n\u001B[0;32m      5\u001B[0m filtered_words \u001B[38;5;241m=\u001B[39m [word \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m word_list \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(word) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m max_length]\n\u001B[1;32m----> 6\u001B[0m env \u001B[38;5;241m=\u001B[39m \u001B[43mHangedManEnv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfiltered_words\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Nastasia_reinforcement_learning_guide\\demos\\hangman\\Hangman_env.py:30\u001B[0m, in \u001B[0;36mHangedManEnv.__init__\u001B[1;34m(self, word_dictionary, max_word_size, render_mode)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, word_dictionary, max_word_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, render_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     27\u001B[0m \n\u001B[0;32m     28\u001B[0m     \u001B[38;5;66;03m# Specifics attributes\u001B[39;00m\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_total_try_games \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_total_try_games\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m---> 30\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoded_state \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempty_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoded_aim \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mempty_like()\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoded_tried_letters \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: empty_like() missing 1 required positional argument: 'prototype'"
     ]
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "word_list = words.words()  # this returns a list\n",
    "max_length = 3\n",
    "filtered_words = [word for word in word_list if len(word) <= max_length]\n",
    "env = HangedManEnv(filtered_words, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# La partie apprentissage"
   ],
   "metadata": {
    "id": "Tfjz3GQjwIz1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DÃ©finission d'une policy"
   ],
   "metadata": {
    "id": "5uTT5aawxozU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qS0oNS2msKvx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Q-learning parameters\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "initial_epsilon = 1.0\n",
    "epsilon_decay = 0.9975\n",
    "min_epsilon = 0.05\n",
    "\n",
    "# Initialize the Q-table\n",
    "# The shape is based on the observation space and action space\n",
    "q_table_shape = (28**max_length, 26)  # Adjust based on the state and action space\n",
    "q_table = np.zeros(q_table_shape)\n",
    "\n",
    "def epsilon_greedy_policy(state):\n",
    "    \"\"\"\n",
    "    Returns an action based on epsilon-greedy policy.\n",
    "    \"\"\"\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        # Exploration: choose a random action\n",
    "        return np.random.choice(26)\n",
    "    else:\n",
    "        # Exploitation: choose the action with max Q-value for the current state\n",
    "        # Mask forbidden actions using the second element of the observation\n",
    "        masked_q_values = np.copy(q_table[state[0]])\n",
    "        # Mask by setting very low Q-value for letter already tried\n",
    "        masked_q_values[state[1]] = -np.inf\n",
    "        # return the action with the max Q-value\n",
    "        return np.argmax(masked_q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entrainement"
   ],
   "metadata": {
    "id": "piF8svfHxvx6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bs_EWNOhsKvy"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# Training parameters\n",
    "total_episodes = 100000\n",
    "batch_size = 500  # Number of episodes in a batch\n",
    "n_tests = 500  # Number of evaluation runs after each batch\n",
    "batch_rewards = []\n",
    "\n",
    "epsilon = initial_epsilon # epsilon will update after each batch\n",
    "\n",
    "# Train per batches\n",
    "# epsilon decay after batch\n",
    "# tests after batch\n",
    "for batch in range(total_episodes // batch_size):\n",
    "    # Training episodes\n",
    "    batch_reward_sum = 0\n",
    "    for episode in range(batch_size):\n",
    "\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        # Convert state to a single integer for indexing in Q-table\n",
    "        state_index = np.dot(state[0], [28**i for i in range(max_length)])\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            # action is determined with epsilon greedy policy\n",
    "            # either exploratory or best according to current q-table\n",
    "            action = epsilon_greedy_policy((state_index, state[1]))\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            # Convert next_state to a single integer for indexing in Q-table\n",
    "            next_state_index = np.dot(next_state[0], [28**i for i in range(max_length)])\n",
    "\n",
    "            # Q-learning update rule\n",
    "            best_next_action = np.argmax(q_table[next_state_index])\n",
    "            td_target = reward + discount_factor * q_table[next_state_index][best_next_action]\n",
    "            td_error = td_target - q_table[state_index][action]\n",
    "            q_table[state_index][action] += learning_rate * td_error\n",
    "\n",
    "            state = next_state\n",
    "            state_index = next_state_index\n",
    "\n",
    "\n",
    "    epsilon *= epsilon_decay\n",
    "    epsilon = max(epsilon, min_epsilon)\n",
    "\n",
    "    # Evaluation after each batch\n",
    "    total_test_rewards = []\n",
    "    for _ in range(n_tests):\n",
    "        state, _ = env.reset()\n",
    "        state_index = np.dot(state[0], [28**i for i in range(max_length)])\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Always choose the best possible action\n",
    "            # Same as using the policy with epsilon = 0\n",
    "            masked_q_values = np.copy(q_table[state_index])\n",
    "            masked_q_values[state[1]] = -np.inf\n",
    "            action = np.argmax(masked_q_values)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            state_index = np.dot(state[0], [28**i for i in range(max_length)])\n",
    "            total_reward += reward\n",
    "\n",
    "        total_test_rewards.append(total_reward)\n",
    "    #\n",
    "    mean_test_reward = np.mean(total_test_rewards)\n",
    "    batch_rewards.append(mean_test_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dgv6JEMsKvy",
    "outputId": "a7ee9d6b-c02b-46b5-f218-9b01eb3ac66b"
   },
   "outputs": [],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xskzr9EcsKvy",
    "outputId": "c8487cf1-6813-4e6f-9dde-6eb0702c2053"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting\n",
    "plt.plot(batch_rewards)\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Mean Evaluation Reward')\n",
    "plt.title('Evaluation Reward over Time')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
