{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training with greedy algorithm.\n"
   ],
   "metadata": {
    "id": "5GLPgVs-t_RD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vKOVSxN3sKvu",
    "ExecuteTime": {
     "end_time": "2023-10-15T16:38:16.927913700Z",
     "start_time": "2023-10-15T16:38:16.911375800Z"
    }
   },
   "outputs": [],
   "source": [
    "from Hangman_env import HangedManEnv\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's start an Environment"
   ],
   "metadata": {
    "id": "AJwo73Olyx5Q"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97ifKg2ssKvx",
    "outputId": "0b0d5a49-bb40-4d57-94f9-c57303e34f45",
    "ExecuteTime": {
     "end_time": "2023-10-15T16:38:20.627697400Z",
     "start_time": "2023-10-15T16:38:20.596350900Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HangedManEnv.__init__() missing 1 required positional argument: 'word_dictionary'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m max_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m\n\u001B[1;32m----> 2\u001B[0m env \u001B[38;5;241m=\u001B[39m \u001B[43mHangedManEnv\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: HangedManEnv.__init__() missing 1 required positional argument: 'word_dictionary'"
     ]
    }
   ],
   "source": [
    "max_length = 3\n",
    "env = HangedManEnv( )"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# La partie apprentissage"
   ],
   "metadata": {
    "id": "Tfjz3GQjwIz1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DÃ©finission d'une policy"
   ],
   "metadata": {
    "id": "5uTT5aawxozU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qS0oNS2msKvx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Q-learning parameters\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "initial_epsilon = 1.0\n",
    "epsilon_decay = 0.9975\n",
    "min_epsilon = 0.05\n",
    "\n",
    "# Initialize the Q-table\n",
    "# The shape is based on the observation space and action space\n",
    "q_table_shape = (28**max_length, 26)  # Adjust based on the state and action space\n",
    "q_table = np.zeros(q_table_shape)\n",
    "\n",
    "def epsilon_greedy_policy(state):\n",
    "    \"\"\"\n",
    "    Returns an action based on epsilon-greedy policy.\n",
    "    \"\"\"\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        # Exploration: choose a random action\n",
    "        return np.random.choice(26)\n",
    "    else:\n",
    "        # Exploitation: choose the action with max Q-value for the current state\n",
    "        # Mask forbidden actions using the second element of the observation\n",
    "        masked_q_values = np.copy(q_table[state[0]])\n",
    "        # Mask by setting very low Q-value for letter already tried\n",
    "        masked_q_values[state[1]] = -np.inf\n",
    "        # return the action with the max Q-value\n",
    "        return np.argmax(masked_q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entrainement"
   ],
   "metadata": {
    "id": "piF8svfHxvx6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bs_EWNOhsKvy"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# Training parameters\n",
    "total_episodes = 100000\n",
    "batch_size = 500  # Number of episodes in a batch\n",
    "n_tests = 500  # Number of evaluation runs after each batch\n",
    "batch_rewards = []\n",
    "\n",
    "epsilon = initial_epsilon # epsilon will update after each batch\n",
    "\n",
    "# Train per batches\n",
    "# epsilon decay after batch\n",
    "# tests after batch\n",
    "for batch in range(total_episodes // batch_size):\n",
    "    # Training episodes\n",
    "    batch_reward_sum = 0\n",
    "    for episode in range(batch_size):\n",
    "\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        # Convert state to a single integer for indexing in Q-table\n",
    "        state_index = np.dot(state[0], [28**i for i in range(max_length)])\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            # action is determined with epsilon greedy policy\n",
    "            # either exploratory or best according to current q-table\n",
    "            action = epsilon_greedy_policy((state_index, state[1]))\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            # Convert next_state to a single integer for indexing in Q-table\n",
    "            next_state_index = np.dot(next_state[0], [28**i for i in range(max_length)])\n",
    "\n",
    "            # Q-learning update rule\n",
    "            best_next_action = np.argmax(q_table[next_state_index])\n",
    "            td_target = reward + discount_factor * q_table[next_state_index][best_next_action]\n",
    "            td_error = td_target - q_table[state_index][action]\n",
    "            q_table[state_index][action] += learning_rate * td_error\n",
    "\n",
    "            state = next_state\n",
    "            state_index = next_state_index\n",
    "\n",
    "\n",
    "    epsilon *= epsilon_decay\n",
    "    epsilon = max(epsilon, min_epsilon)\n",
    "\n",
    "    # Evaluation after each batch\n",
    "    total_test_rewards = []\n",
    "    for _ in range(n_tests):\n",
    "        state, _ = env.reset()\n",
    "        state_index = np.dot(state[0], [28**i for i in range(max_length)])\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Always choose the best possible action\n",
    "            # Same as using the policy with epsilon = 0\n",
    "            masked_q_values = np.copy(q_table[state_index])\n",
    "            masked_q_values[state[1]] = -np.inf\n",
    "            action = np.argmax(masked_q_values)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            state_index = np.dot(state[0], [28**i for i in range(max_length)])\n",
    "            total_reward += reward\n",
    "\n",
    "        total_test_rewards.append(total_reward)\n",
    "    #\n",
    "    mean_test_reward = np.mean(total_test_rewards)\n",
    "    batch_rewards.append(mean_test_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dgv6JEMsKvy",
    "outputId": "a7ee9d6b-c02b-46b5-f218-9b01eb3ac66b"
   },
   "outputs": [],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xskzr9EcsKvy",
    "outputId": "c8487cf1-6813-4e6f-9dde-6eb0702c2053"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting\n",
    "plt.plot(batch_rewards)\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Mean Evaluation Reward')\n",
    "plt.title('Evaluation Reward over Time')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
